



ComputeFugly( A, b :: Vector, Î› ) = sum( A[k,:] * b[k] /  Î›[k] for k âˆˆ eachindex( Î› ) )
ComputeFugly( A, B, Î› ) = sum( A[k,:] * B[k,:]' /  Î›[k] for k âˆˆ eachindex( Î› ) )
ComputeFuglyT( A, b :: Vector, Î› ) = sum( A[:,k] * b[k] /  Î›[k] for k âˆˆ eachindex( Î› ) )
ComputeFuglyT( A, B, Î› ) = sum( A[:,k] * B[k,:]' /  Î›[k] for k âˆˆ eachindex( Î› ) )

function CombineToFormp!( p, Q, Qáµ€g, Qáµ€K, Î›, r )
    p[:] = - ComputeFuglyT( Q, Qáµ€g, Î› ) + ComputeFuglyT( Q, Qáµ€K * r, Î› )
end



# equation (4.38) in Nocedahl and Wright
function DetermineDirection!( p, Î», Q, Qáµ€g, Qáµ€K, Î› )
    Î›Ê³ = ( iszero( Î» ) ? Î› : [ Î›[m] .+ Î» for m âˆˆ eachindex( Î› ) ] )
    V = inv( ThreadsX.mapreduce( (x,y,z)-> ComputeFugly( x, y, z ), +, Qáµ€K, Qáµ€K, Î›Ê³ ) + I )    # (I + K'H^{-1}K)^{-1}
    r = V * ThreadsX.mapreduce( (x,y,z) -> ComputeFugly( x, y, z ), +, Qáµ€K, Qáµ€g, Î›Ê³  )         # (I + K'H^{-1}K)^{-1} K'H^{-1}g
    ThreadsX.map( (a,b,c,d,e) -> CombineToFormp!( a, b, c, d, e, r ), p, Q, Qáµ€g, Qáµ€K, Î›Ê³ )    # -HÊ³^{-1} g
    return p
end


function Computem( g :: VVector{T}, H, K, p ) where {T<:Flt}
    mval = ThreadsX.mapreduce( (x,y,z) -> dot( x, z ) + 0.5 * dot( z, y * z ), +, g, H, p )
    KË¢ = sum( K[m]' * p[m] for m âˆˆ eachindex( p ) )
    return mval + T(0.5) * dot( KË¢, KË¢ )
end



function CholeskyOfRidge!( HÊ³, H, Î» )
    for i âˆˆ axes( HÊ³, 1 )
        HÊ³[i,i] = H[i,i] + Î»
    end
    return cholesky( Hermitian( HÊ³); check = false )
end


function Assign_to_p!( p, R, y, z )
    p .= - R \ ( y - z )
    return nothing
end

q22sum( Râ±½, A, Rp ) = sum( abs, Râ±½' * A' * Rp )


Norm( v :: VVector{T} ) where {T<:Flt} = sqrt( sum( sum.( abs2, v ) ) )

function NewterInitialSafeguards( g :: VVector{T}, H :: VMatrix{T}, Î”, Î» ) where {T<:Flt}
    Î»Ë¢ = maximum( ThreadsX.map( x->maximum( -diag( x ) ), H ) )
    g_norm = Norm( g ) 
    H_norm = maximum( opnorm.( H ) )
    Î»Ì² = max( zero( T ), Î»Ë¢, g_norm / Î” - H_norm )
    Î»Ì„ = g_norm / Î” + H_norm
    Î» = min( max( Î», Î»Ì² ), Î»Ì„ )
    Î» > Î»Ë¢ && return Î»
    return max( one( T ) * 0.001 * Î»Ì„, sqrt( Î»Ì„ * Î»Ì² ) )
end


function Newter_SolveSubproblem!(
    p :: VVector{ T },          # storage for direction vectors
    g :: VVector{ T },          # complete gradient (include PLM portion)
    H :: VMatrix{ T },          # Likelihood hessian 
    K :: VMatrix{ T },          # Likelihood K
    Î” :: T,                     # trust region size
    Ï„ = 1e-10,                  # tolerance
    Iâº = 5                      # maximum number of iterations
    ) where {T <: Flt}


    ğ“‚, interior, Î», ğŸ’¼, ğŸ¾ = T( Inf ), false, zero( T ), false, false

    failure = ğ“‚, interior, Î», ğŸ’¼, ğŸ¾
    Î”Â² = Î”^2

    Hsym = Symmetric.( H )
    any( any.(!isfinite, Hsym) ) && begin 
            @debug "infinite Hsym"
            return failure
        end 
    Heig = ThreadsX.map( x->eigen( x ), Hsym )

    any( map( x->isempty( x.values ), Heig ) ) &&  begin 
            @debug "empty eigenvalue vector"
            return failure
        end
    Î› = ThreadsX.map( x->x.values, Heig )      
    Q = ThreadsX.map( x->x.vectors, Heig )
    minHev = minimum( minimum.( Î› ) )
    maxHev = maximum( maximum.( Î› ) )


    HÊ³ = copy.( H )

    Qáµ€g = ThreadsX.map( (x,y)->x' * y, Q, g )
    Qáµ€K = ThreadsX.map( (x,y)->x' * y, Q, K )
              # vector of all eigenvalues of H

    # check if the unconstrained solution works
    if minHev â‰¥ 1e-8 
        DetermineDirection!( p, zero(T), Q, Qáµ€g, Qáµ€K, Î› )
        sum( sum.(abs2, p) ) â‰¤ Î”Â² && return Computem( g, H, K, p ), true, Î», ğŸ’¼, true 
    end

    @ensure minHev â‰¥ zero( T ) "hard case not programmed up"

    Î»Ì² = nextfloat( -minHev )
    Î» = NewterInitialSafeguards( g, H, Î”, Î» )

    interior = ğŸ¾ = false
    for ğ’¾ âˆˆ 1:Iâº
        Î»â» = Î»
        ğ’  = ThreadsX.map( (x,y)->CholeskyOfRidge!( x, y, Î» ), HÊ³, H )
        if !all( issuccess.( ğ’ ) )
            Î» *= 2.0
            continue
        end
        R = [ C.U for C âˆˆ ğ’ ]
        A = ThreadsX.map( (x,y)->x'\y, R, K )
        s = ThreadsX.map( (x,y)->x'\y, R, g )
        V = inv( ThreadsX.mapreduce( x-> x'x, +, A ) + I )
        r = ThreadsX.mapreduce( (X,y) -> X' * y, +, A, s )
        ThreadsX.map( (p,R,s,A) -> Assign_to_p!( p, R, s, A * r ), p, R, s, A )
        # @debug "$Î» $p"
        p2 = sum( sum.( abs2, p ) )
        Râ±½ = cholesky( Symmetric( V ) ).U 
        Rp = ThreadsX.map( (R,p) -> R'\p, R, p )
        qâ‚Â² = sum( sum.( abs2, Rp ) )
        qâ‚‚Â² = ThreadsX.mapreduce( (A,Rp) -> q22sum( Râ±½,A, Rp ), +, A, Rp )
        q2 = qâ‚Â² + qâ‚‚Â²
        Î»âº = p2 * ( sqrt(p2) - Î” ) / ( Î” * q2 )
        Î» += Î»âº

        Î» < Î»Ì² && (  Î» = 0.5 * ( Î»â» - Î»Ì² ) + Î»Ì² )
        # @debug "lambda and its bounds:  $Î»Ì² â‰¤ $Î»     previous = $Î»â»"
        abs( Î» - Î»â» ) < Ï„ && return Computem( g, H, K, p ), false, Î», ğŸ’¼, true
    end

    # @debug "run out of iterations"
    return Computem( g, H, K, p ), false, Î», ğŸ’¼, false
end


NewterLoss( state, f = abs2 ) =  mapreduce( (x,px)->sum( f, x - px ), +, state.x, state.x_previous )
    

function NewterUpdateState!( ğŸ¦ :: Newter{T}, state :: NewterState{T}, method :: NewterMethod{T} ) where {T<:Flt}
    # fine the next step direction
    # @debug "initial gradient = $(gradient(ğŸ¦))"
    # @debug "initial hessian = $(hessian(ğŸ¦))"
    ( â„³, state.interior, state.Î», state.reached_subproblem_solution ) = 
    Newter_SolveSubproblem!( state.p, gradient( ğŸ¦ ), hessian( ğŸ¦ ), Kmat( ğŸ¦ ), state.Î” )

    # maintain a record of the previous position
    deepcopyto!( state.x_previous, state.x )
    state.f_x_previous = value( ğŸ¦ )

    # update the function value and gradient
    # @debug "from $(state.x)"
    # @debug "p=$(state.p)"
    map!( (x,y)->x+y, state.x, state.x, state.p )
    # @debug "to $(state.x)"
    # exit()
    deepcopyto!( state.g_previous, gradient( ğŸ¦ ) )
    value_gradient!( ğŸ¦, state.x )         

    # update the trust region size (algorithm 4.1 in N&W 2006)
    f_x_diff = state.f_x_previous - value( ğŸ¦ )
    state.Ï = ( abs( â„³ ) â‰¤ eps( T ) ) ?  one( T ) : ( ( â„³ > 0 ) ? -one(T) : f_x_diff / ( 0 - â„³ ) )

    if state.Ï < method.ÏÌ²
        # @info "Ï less than lower bound, reducing Î”"
        state.Î” *= 0.25
    elseif state.Ï > method.ÏÌ„ && !state.interior
        # @info "Ï above upper and not in the interior "
        state.Î” = min( 2 * state.Î”, method.Î”Ì‚ )
    else
        # @info "default situation"
    end

    if state.Ï â‰¤ state.Î·
        # The improvement is too small and we won't take it.
        # If you reject an interior solution, make sure that the next
        # delta is smaller than the current step. Otherwise you waste
        # steps reducing delta by constant factors while each solution
        # will be the same.
        state.Î” = 0.25 * sqrt( NewterLoss( state ) )

        ğŸ¦.F = state.f_x_previous
        deepcopyto!( state.x, state.x_previous )
        deepcopyto!( ğŸ¦.DF, state.g_previous )
        deepcopyto!( ğŸ¦.x_df, state.x_previous )
    else
        hessian!( ğŸ¦, state.x )
    end

    return nothing
end





function NewterAssessConvergence( state::NewterState{T}, ğŸ¦::Newter{T}, options::NewterOptions{T} ) where {T<:Flt}
    state.Ï â‰¤ state.Î· && return false, false, false, false
    # Accept the point and check convergence
    x_converged = ( NewterLoss( state, abs ) â‰¤ options.x_abs_tol ) 
    # @debug "x convergence: $(NewterLoss( state, abs ))  desired â‰¤ $(options.x_abs_tol)"
    f_converged = ( abs( value( ğŸ¦ ) - state.f_x_previous ) â‰¤ options.f_rel_tol * abs( value( ğŸ¦ ) ) )
    # @debug "f convergence: $( abs( value( ğŸ¦ ) - state.f_x_previous ))   desired â‰¤ $(options.f_rel_tol * abs( value( ğŸ¦ ) ) )"
    f_increased = ( value( ğŸ¦ ) > state.f_x_previous )
    g_converged = ( maximum( maximum.( abs, gradient( ğŸ¦ ) ) )  â‰¤ options.g_abs_tol )
    # g_converged = ( sum( sum.( abs, gradient( ğŸ¦ ) ) )  â‰¤ options.g_abs_tol )
    @debug "g convergence: $( maximum( maximum.( abs, gradient( ğŸ¦ ) ) ))  desired  â‰¤ $(options.g_abs_tol )"

    return x_converged, f_converged, g_converged, f_increased
end



function NewterInitialState( method :: NewterMethod{T}, ğŸ¦ :: Newter{T}, initial_x :: VVector{ T } ) where { T<: Flt }
    n = length(initial_x)
    # Maintain current gradient in gr
    @assert( method.Î”Ì‚ > 0, "Î”hat must be strictly positive" )
    @assert( 0 < method.initial_Î” < method.Î”Ì‚, "Î” must be in (0, Î”Ì‚)")
    @assert(0 <= method.Î· < method.ÏÌ², "Î· must be in [0, Ï_lower)")
    @assert(method.ÏÌ² < method.ÏÌ„, "must have Ï_lower < Ï_upper")
    @assert(method.ÏÌ² â‰¥ 0.)
    # Keep track of trust region sizes
    Î” = method.initial_Î”

    # Record attributes of the subproblem in the trace.
    reached_subproblem_solution = true
    interior = true
    Î» = T( NaN )

    value_gradient_hessian!!( ğŸ¦, initial_x )

    return NewterState( 
        deepcopy( initial_x ),
        deepcopy( initial_x ),
        T(Inf),
        deepcopy( gradient( ğŸ¦ ) ),
        deepcopy( gradient( ğŸ¦ ) ),
        T( Î” ),
        zero( T ),
        Î»,
        T( method.Î· ),
        interior,
        reached_subproblem_solution
        )
end


function NewterTrace!( tr::NewterTrace{T}, ğŸ¦::Newter{T}, state::NewterState{T}, iteration::Int, method::NewterMethod{T}, options::NewterOptions{T}, curr_time=time() ) where {T<:Flt}
    dt = Dict()
    dt["time"] = curr_time - tr.starttime
    if options.extended_trace
        dt["x"] = deepcopy( state.x )
        dt["g(x)"] = deepcopy( gradient(ğŸ¦) )
        dt["h(x)"] = deepcopy( hessian(ğŸ¦) )
        dt["delta"] = state.Î”
        dt["interior"] = state.interior
        dt["hard case"] = false
        dt["reached_subproblem_solution"] = state.reached_subproblem_solution
        dt["lambda"] = state.Î»
    end
    g_norm = maximum( maximum.( gradient( ğŸ¦ )  ) )
    NewterUpdateTrace!(tr, iteration, value( ğŸ¦ ), state.x, g_norm, dt )
end


NewterUpdateTrace!( tr :: NewterTrace{T}, iteration, f, x, gnorm, dt ) where {T<:Flt} = push!( tr.tr, NewterTrace1( iteration, f, x, gnorm, dt ) )



function NewterBest!( ğŸ¦ :: Newter{T}, best :: PMLFGH{T} ) where {T<:Flt}
    ğŸ¦.F â‰¤ best.F[1] || return
    best.F[1] = ğŸ¦.F
    g = gradient( ğŸ¦ )
    H = hessian( ğŸ¦ )
    for m âˆˆ eachindex( best.market )
        copyto!( best.market[m].inside.GÎ´, g[m] )
        copyto!( best.market[m].inside.HÎ´Î´, H[m] )
        copyto!( best.market[m].Î´, ğŸ¦.x_f[m] )
    end
    return nothing
end



