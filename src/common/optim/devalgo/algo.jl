




ComputeFugly( A, B, Œõ ) = sum( A[k,:] * B[k,:]' /  Œõ[k] for k ‚àà eachindex( Œõ ) )

function CombineToFormp!( p, Q, Q·µÄg, Q·µÄK, Œõ, r )
    p[:] = - ComputeFugly( Q, Q·µÄg, Œõ ) + ComputeFugly( Q, Q·µÄK * r, Œõ )
end

# equation (4.38) in Nocedahl and Wright
function DetermineDirection!( p, Œª, Q, Q·µÄg, Q·µÄK, Œõ )
    for m ‚àà eachindex( Œõ )
        Œõ[m] .+= Œª                                                                                # ridgefy
    end
    V = inv( ThreadsX.mapreduce( (x,y,z)-> ComputeFugly( x, y, z ), +, Q·µÄK, Q·µÄK, Œõ ) + I )    # (I + K'H^{-1}K)^{-1}
    r = V * ThreadsX.mapreduce( (x,y,z) -> ComputeFugly( x, y, z ), +, Q·µÄK, Q·µÄg, Œõ  )         # (I + K'H^{-1}K)^{-1} K'H^{-1}g
    ThreadsX.map( (a,b,c,d,e) -> CombineToFormp!( a, b, c, d, e, r ), p, Q, Q·µÄg, Q·µÄK, Œõ )    # -H ≥^{-1} g
    return p
end


function Computem( g, H, K, p )
    mval = ThreadsX.mapreduce( (x,y,z) -> dot( x, z ) + 0.5 * dot( z, y * z ), +, g, H, p )
    KÀ¢ = sum( K[m]' * p[m] for m ‚àà eachindex( p ) )
    return mval + T(0.5) * dot( KÀ¢, KÀ¢ )
end



function CholeskyOfRidge!( H ≥, H, Œª )
    for i ‚àà axes( H ≥, 1 )
        H ≥[i,i] = H[i,i] + Œª
    end
    return cholesky( Hermitian( H ≥); check = false )
end


function Assign_to_p!( p, R, y, z )
    p .= R \ ( y - z )
    return p
end

q22sum( R‚±Ω, A, Rp ) = sum( abs, R‚±Ω' * A' * Rp )

import LinearAlgebra.norm

norm( v :: VVector{T} ) where {T<:Flt} = sqrt( sum( sum.( abs2, g ) ) )

function Newter_initial_safeguards( g :: VVector{T}, H :: VMatrix{T}, Œî, Œª ) where {T<:Flt}
    ŒªÀ¢ = ThreadsX.mapreduce( x->maximum( -diag( x ) ), maximum, H )
    g_norm = norm( g ) 
    H_norm = sum( sum.( abs, H ) )
    ŒªÃ≤ = max( zero( T ), ŒªÀ¢, g_norm / Œî - H_norm )
    ŒªÃÑ = g_norm / Œî + H_norm
    Œª = min( max( Œª, ŒªÃ≤ ), ŒªÃÑ )
    Œª > ŒªÀ¢ && return Œª
    return max( one( T ) * 0.001 * ŒªÃÑ, sqrt( ŒªÃÑ * ŒªÃ≤ ) )
end


function Newter_SolveSubproblem!(
    p :: VVector{ T };          # storage for direction vectors
    g :: VVector{ T },          # complete gradient (include PLM portion)
    H :: VMatrix{ T },          # Likelihood hessian 
    K :: VMatrix{ T },          # Likelihood K
    Œî :: T,                     # trust region size
    œÑ = 1e-10,                  # tolerance
    I‚Å∫ = 5                      # maximum number of iterations
    ) where {T <: Flt}

    @info "solving subproblem"

    ùìÇ, interior, Œª, üíº, üçæ = T( Inf ), false, zero( T ), false, false

    failure = ùìÇ, interior, Œª, üíº, üçæ
    Œî¬≤ = Œî^2

    Hsym = Symmetric.( H )
    any( any.(!isfinite, Hsym) ) && return failure
    Heig = ThreadsX.map( x->eigen( x ), Hsym )

    any( map( x->isempty( x.values ), Heig ) ) &&  return failure
    Œõ = ThreadsX.map( x->x.values, Heig )      
    Q = ThreadsX.map( x->x.vectors, Heig )
    minHev = minimum( minimum.( Œõ ) )
    maxHev = maximum( maximum.( Œõ ) )


    H ≥ = copy( H )

    Q·µÄg = ThreadsX.map( (x,y)->x' * y, Q, g )
    Q·µÄK = ThreadsX.map( (x,y)->x' * y, Q, K )
              # vector of all eigenvalues of H

    # check if the unconstrained solution works
    if minHev ‚â• 1e-8 
        DetermineDirection!( p, zero(T), Q, Q·µÄg, Q·µÄK, Œõ )
        sum( sum.(abs2, p) ) ‚â§ Œî¬≤ && return Computem( g, H, K, p ), true, Œª, üíº, true 
    end

    @ensure minHev ‚â• zero( T ) "hard case not programmed up"

    ŒªÃ≤ = nextfloat( -minHev )
    Œª = Newter_initial_safeguards( g, H, Œî, Œª )

    interior = üçæ = false
    for ùíæ ‚àà 1:I‚Å∫
        @debug "$ùíæ $Œª"
        Œª‚Åª = Œª
        ùíû  = ThreadsX.map( (x,y)->CholeskyOfRidge!( x, y, Œª ), H ≥, H )
        if !all( issuccess.( ùíû ) )
            Œª *= 2.0
            continue
        end

        R = [ C.U for C ‚àà ùíû ]
        A = ThreadsX.map( (x,y)->x'\y, R, K )
        s = ThreadsX.map( (x,y)->x'\y, R, g )
        V = inv( ThreadsX.mapreduce( x-> x'x, +, A ) + I )
        r = ThreadsX.mapreduce( (X,y) -> X' * y, A, s )
        ThreadsX.map( (x,Y,z,a) -> Assign_to_p!( x, Y, z, a ), p, R, s, r )
        p2 = sum( sum.( abs2, p ) )
        R‚±Ω = cholesky( V ).U 
        Rp = ThreadsX.map( (R,p) -> R'\p, R, p )
        q‚ÇÅ¬≤ = sum( sum.( abs2, Rp ) )
        q‚ÇÇ¬≤ = ThreadsX.mapreduce( (A,Rp) -> q22sum( R‚±Ω,A, Rp ), +, A, Rp )
        q2 = q‚ÇÅ¬≤ + q‚ÇÇ¬≤
        Œª‚Å∫ = p2 * ( sqrt(p2) - Œî ) / ( Œî * q2 )
        Œª += Œª‚Å∫

        Œª < ŒªÃ≤ && (  Œª = 0.5 * ( Œª‚Çã - ŒªÃ≤ ) + ŒªÃ≤ )

        if abs( Œª - Œª‚Çã ) < œÑ
            üçæ = true
            return Computem( g, H, K, p ), false, Œª, üíº, true 
        end
    end

    return Computem( g, H, K, p ), false, Œª, üíº, false
end


NewterLoss( state, f = abs2 ) =  mapreduce( state->sum( f, state.x - state.x_previous ), +, state )
    

function NewterUpdateState!( newt :: Newter{T}, state :: NewterState{T}, method :: NewterMethod{T} ) where {T<:Flt}
    # fine the next step direction
    ( ‚Ñ≥, state.interior, state.Œª, state.reached_subproblem_solution ) = 
    Newter_SolveSubproblem!( state.p, gradient( newt ), Hessian( newt ), Kmat( newt ), state.Œî )

    # maintain a record of the previous position
    deepcopyto!( state.x_previous, state.x )
    state.f_x_previous = value( newt )

    # update the function value and gradient
    map!( x->x+y, state.x, state.x, state.s )
    deepcopyto!( state.g_previous, gradient( newt ) )
    value_gradient!( newt, state.x )         

    # update the trust region size (algorithm 4.1 in N&W 2006)
    f_x_diff = state.f_x_previous - value( newt )
    state.œÅ = ( abs( ‚Ñ≥ ) ‚â§ eps( T ) ) ?  one( T ) : ( ( ‚Ñ≥ > 0 ) ? -one(T) : f_x_diff / ( 0 - ‚Ñ≥ ) )

    if state.œÅ < method.œÅÃ≤
        # @info "œÅ less than lower bound, reducing Œî"
        state.Œî *= 0.25
    elseif state.œÅ > method.œÅÃÑ && !state.interior
        # @info "œÅ above upper and not in the interior "
        state.Œî = min( 2 * state.Œî, method.ŒîÃÇ )
    else
        # @info "default situation"
    end

    if state.œÅ ‚â§ state.Œ∑
        # The improvement is too small and we won't take it.
        # If you reject an interior solution, make sure that the next
        # delta is smaller than the current step. Otherwise you waste
        # steps reducing delta by constant factors while each solution
        # will be the same.
        state.Œî = 0.25 * sqrt( NewterLoss( state) )

        newt.F = state.f_x_previous
        deepcopyto!( state.x, state.x_previous )
        deepcopyto!( newt.DF, state.g_previous )
        deepcopyto!( newt.x_df, state.x_previous )
    else
        hessian!( newt, state.x )
    end

    return nothing
end





function NewterAssessConvergence( state::NewterState{T}, newt::Newter{T}, options::NewterOptions{T} ) where {T<:Flt}
    state.œÅ ‚â§ state.Œ∑ && return false, false, false, false
    # Accept the point and check convergence
    x_converged = ( NewterLoss( state, abs ) ‚â§ options.x_abs_tol ) 
    f_converged = ( abs( value( newt ) - state.f_x_previous ) ‚â§ options.f_rel_tol * abs( value( newt ) ) )
    f_increased = ( value( newt ) > state.f_x_previous )
    g_converged = ( sum( sum.( abs, gradient( newt ) ) )  ‚â§ options.g_abs_tol )
    return x_converged, f_converged, g_converged, f_increased
end



function NewterInitialState( method :: NewterMethod{T}, newt :: Newter{T}, initial_x :: VVector{ T } ) where { T<: Flt }
    n = length(initial_x)
    # Maintain current gradient in gr
    @assert( method.Œîhat > 0, "Œîhat must be strictly positive" )
    @assert( 0 < method.initial_Œî < method.Œîhat, "Œî must be in (0, Œîhat)")
    @assert(0 <= method.Œ∑ < method.œÅ_lower, "Œ∑ must be in [0, œÅ_lower)")
    @assert(method.œÅ_lower < method.œÅ_upper, "must have œÅ_lower < œÅ_upper")
    @assert(method.œÅ_lower ‚â• 0.)
    # Keep track of trust region sizes
    Œî = method.initial_Œî

    # Record attributes of the subproblem in the trace.
    reached_subproblem_solution = true
    interior = true
    Œª = T( NaN )

    value_gradient_hessian!!( newt, initial_x )

    return NewterState( 
        deepcopy( initial_x ),
        deepcopy( initial_x ),
        T(Inf),
        deepcopy( gradient( newt ) ),
        deepcopy( gradient( newt ) ),
        T( Œî ),
        zero( T ),
        Œª,
        T( method.Œ∑ ),
        interior,
        reached_subproblem_solution
        )
end


function NewterTrace!( tr::NewterTrace{T}, newt::Newter{T}, state::NewterState{T}, iteration::Int, method::NewterMethod{T}, options::NewterOptions{T}, curr_time=time() ) where {T<:Flt}
    dt = Dict()
    dt["time"] = curr_time - tr.starttime
    if options.extended_trace
        dt["x"] = deepcopy( state.x )
        dt["g(x)"] = deepcopy( gradient(newt) )
        dt["h(x)"] = deepcopy( hessian(newt) )
        dt["delta"] = state.Œî
        dt["interior"] = state.interior
        dt["hard case"] = false
        dt["reached_subproblem_solution"] = state.reached_subproblem_solution
        dt["lambda"] = state.Œª
    end
    g_norm = ntr_loss( gradient( newt ) )
    NewterUpdateState!(tr, iteration, value( newt ), state.x, g_norm, dt )
end





function NewterBest!( newt :: Newter{T}, best :: PMLFGH{T} ) where {T<:Flt}
    newt.F ‚â§ best.F[1] || return
    best.F[1] = ntr.F
    g = gradient( ntr )
    H = hessian( ntr )
    for m ‚àà eachindex( best.market )
        copyto!( best.market[m].inside.GŒ¥, g[m] )
        copyto!( best.market[m].inside.HŒ¥Œ¥, H[m] )
        copyto!( best.market[m].Œ¥, ntr.x_f[m] )
    end
    return nothing
end
